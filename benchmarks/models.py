import os
import sys
import json
import codecs
import subprocess
import time
import torch

from transformers import AutoTokenizer, AutoModelForCausalLM, RobertaTokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM


access_token = "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"


class Model:
    def __init__(self):
        pass


CodeGenInputConfig = {
    "CODEGEN_COMPLETE_CODEFORM_NOCOMMENT": {
        "model_id": "codegen-350M/2B/6B-multi",
        "input": "buggy function before",
        "patch": "code generated by the model, which will replace the entire buggy function. need extra analysis to figure out where to stop"
    },
    "CODEGEN_COMPLETE_CODEFORM_COMMENTFORM_NOCOMMENT": {
        "model_id": "codegen-350M/2B/6B-multi",
        "input": "buggy function before",
        "patch": "the buggy function before the buggy lines, with buggy lines start with '// buggy line:'. remove all the other commonts and empty lines in the code"
    }
}


class CodeGen(Model):
    def __init__(self, JAVA_DIR, BENCH_DIR):
        super().__init__()
        self.JAVA_DIR = JAVA_DIR
        self.BENCH_DIR = BENCH_DIR

    def command(self, cmd):
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, err = process.communicate()
        if output != b'' or err != b'':
            print(output)
            print(err)
        return output, err

    def get_parsed_input(self, filename, start, end, config, tmp_file):
        os.chdir(self.JAVA_DIR)
        print('java', '-cp', '.:target:lib/*', 'clm.codegen.CodeGenInputParser',
            filename, start, end, config, tmp_file)
        self.command([
            'java', '-cp', '.:target:lib/*', 'clm.codegen.CodeGenInputParser',
            filename, start, end, config, tmp_file
        ])

    def get_input(self, config, output_file, bench_dir):
        if "humaneval" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'humaneval_loc.txt', 'r', 'utf-8')
            codegen_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end
                tmp_file = self.BENCH_DIR + 'tmp/humaneval-java/proj/tmp_codegen.json'
                self.get_parsed_input(
                    bench_dir + 'src/main/java/humaneval/buggy/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', output_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                codegen_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(codegen_input, open(output_file, 'w'), indent=2)

        elif "quixbugs" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'quixbugs_loc.txt', 'r', 'utf-8')
            codegen_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end
                tmp_file = self.BENCH_DIR + 'tmp/quixbugs/proj/tmp_codegen.json'
                self.get_parsed_input(
                    bench_dir + 'java_programs/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', output_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                codegen_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(codegen_input, open(output_file, 'w'), indent=2)

        elif "defects4j" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'defects4j_loc.txt', 'r', 'utf-8')
            codegen_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                proj, bug_id, path, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end
                tmp_dir = self.BENCH_DIR + f'tmp/defects4j/proj/{proj}_{bug_id}/'
                tmp_file = f'{tmp_dir}/tmp_codegen.json'

                print('defects4j', 'checkout', '-p', proj, '-v', bug_id + 'b', '-w', tmp_dir)
                subprocess.call(['defects4j', 'checkout', '-p', proj, '-v', bug_id + 'b', '-w', tmp_dir],
                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                self.get_parsed_input(tmp_dir + path, start, end, config, tmp_file)

                if not os.path.exists(tmp_file):
                    print(proj, bug_id, 'failed.', tmp_file, 'not found.')
                print(proj, bug_id, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                if result['input'].strip() == '':
                    print(proj, bug_id, 'failed. all empty.')

                result = json.load(open(tmp_file, 'r'))
                filename = f"{proj}_{bug_id}_{path}_{rem_loc}"
                codegen_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }

                self.command(['rm', '-rf', tmp_file])
                self.command(['rm', '-rf', tmp_dir])
                json.dump(codegen_input, open(output_file, 'w'), indent=2)
            self.command(['rm', '-rf', self.BENCH_DIR + f'tmp/defects4j/proj/'])

        else:
            raise "Not known benchmark"

    def create_output(self, input_file, output_file, tokenizer_dir, model_dir, model_name, max_new_tokens, num_output=10):
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
        model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True).to("cuda")
        # model = model.to(device_ids[0])

        codegen_output = json.load(open(input_file, 'r'))
        codegen_output['model'] = model_name
        start_time = time.time()
        for i, filename in enumerate(codegen_output['data']):
            text = codegen_output['data'][filename]['input']
            print(i + 1, 'generating', filename)

            try:
                input_ids = tokenizer(text, return_tensors="pt").input_ids.to("cuda")
                if input_ids.size(1) >= int(tokenizer.model_max_length):
                    print('input too long:', input_ids.size(1), 'skip')
                    continue

                eos_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
                generated_ids = model.generate(
                    input_ids, max_new_tokens=max_new_tokens, num_beams=num_output, num_return_sequences=num_output,
                    pad_token_id=eos_id, eos_token_id=eos_id
                )
                output = []
                for generated_id in generated_ids:
                    output.append(tokenizer.decode(generated_id, skip_special_tokens=True))
            except Exception as e:
                print(f"Can't load the model, unexpected exception occured: {e}")
                output = []
            codegen_output['data'][filename]['output'] = output
            json.dump(codegen_output, open(output_file, 'w'), indent=2)
        total_time = int(time.time() - start_time)
        codegen_output['time'] = total_time
        json.dump(codegen_output, open(output_file, 'w'), indent=2)

    @staticmethod
    def output_to_patch(output, config):
        """
        find the } that matches the first { in the output
        """
        output = output.strip().split('\n')
        no_comment_output = [line for line in output if not line.strip().startswith('//')]
        output = '\n'.join(no_comment_output)
        stack = ['{']
        try:
            start_index = output.index('{')
            patch = output[: start_index + 1]
            for c in output[start_index + 1:]:
                patch += c
                if c == '}':
                    top = stack.pop()
                    if top != '{':
                        return ''
                    if len(stack) == 0:
                        return patch.strip()
                elif c == '{':
                    stack.append(c)
            return ''
        except Exception as e:
            return ''

    @staticmethod
    def prepare_input(fn_before, fn_bug, fn_fix, fn_after, eos_token):
        inputs = fn_before
        if fn_bug is not None:
            # Replace all lines with buggy line format
            fn_bug = fn_bug.replace("\n", "\n// buggy line:")

            # Remove last buggy line (For empty line)
            if fn_bug.endswith('// buggy line:'):
                fn_bug = fn_bug[:-len(str('// buggy line:'))]

            # If buggy line is not already included
            if not fn_bug.startswith('\n// buggy line:'):
                inputs += '// buggy line: '

            # Add buggy part and special token
            inputs += fn_bug + "\n"
        #inputs += eos_token
        outputs = fn_fix + fn_after + eos_token
        return inputs, outputs


CodeT5InputConfig = {
    "CODET5_BASE_CODEFORM_MASKFORM_NOCOMMENT": {
        "model_id": "codet5-small/base/large",
        "input": "entire buggy function, with buggy lines masked by <extra_id_0>",
        "patch": "code generated by the model, which will replace the buggy lines"
    },
    "CODET5_BASE_CODEFORM_COMMENTFORM_NOCOMMENT": {
        "model_id": "codet5-small/base/large",
        "input": "entire buggy function, with comments telling the buggy lines and buggy lines masked by <extra_id_0>",
        "patch": "code generated by the model, which will replace the buggy lines"
    },
}

class CodeT5(Model):
    def __init__(self, JAVA_DIR, BENCH_DIR):
        super().__init__()
        self.JAVA_DIR = JAVA_DIR
        self.BENCH_DIR = BENCH_DIR

    def command(self, cmd):
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, err = process.communicate()
        if output != b'' or err != b'':
            print(output)
            print(err)
        return output, err

    def get_parsed_input(self, filename, start, end, config, tmp_file):
        os.chdir(self.JAVA_DIR)
        self.command([
            'java', '-cp', '.:target:lib/*', 'clm.codet5.CodeT5InputParser',
            filename, start, end, config, tmp_file
        ])

    @staticmethod
    def prepare_input(fn_before, fn_bug, fn_fix, fn_after, eos_token):
        inputs = fn_before
        if fn_bug is not None:
            # Replace all lines with buggy line format
            fn_bug = fn_bug.replace("\n", "\n// buggy line:")

            # Remove last buggy line (For empty line)
            if fn_bug.endswith('// buggy line:'):
                fn_bug = fn_bug[:-len(str('// buggy line:'))]

            # If buggy line is not already included
            if not fn_bug.startswith('\n// buggy line:'):
                inputs += '// buggy line: '

            # Add buggy part and special token
            inputs += fn_bug + "<extra_id_0>\n"
        inputs += fn_after
        outputs = fn_fix + eos_token
        return inputs, outputs

    def get_input(self, config, output_file, bench_dir):
        if "humaneval" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'humaneval_loc.txt', 'r', 'utf-8')
            codet5_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end

                tmp_file = self.BENCH_DIR + 'tmp/humaneval-java/proj/tmp_codet5.json'
                self.get_parsed_input(
                    bench_dir + 'src/main/java/humaneval/buggy/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', tmp_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                codet5_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(codet5_input, open(output_file, 'w'), indent=2)

        elif "quixbugs" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'quixbugs_loc.txt', 'r', 'utf-8')
            codet5_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end

                tmp_file = self.BENCH_DIR + 'tmp/quixbugs/proj/tmp_codet5.json'
                self.get_parsed_input(
                    bench_dir + 'java_programs/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', tmp_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                codet5_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(codet5_input, open(output_file, 'w'), indent=2)

        elif "defects4j" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'defects4j_loc.txt', 'r', 'utf-8')
            codet5_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                proj, bug_id, path, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end
                tmp_dir = self.BENCH_DIR + f'tmp/defects4j/proj/{proj}_{bug_id}/'
                tmp_file = f'{tmp_dir}/tmp_codet5.json'

                print('defects4j', 'checkout', '-p', proj, '-v', bug_id + 'b', '-w', tmp_dir)
                subprocess.call(['defects4j', 'checkout', '-p', proj, '-v', bug_id + 'b', '-w', tmp_dir],
                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                self.get_parsed_input(tmp_dir + path, start, end, config, tmp_file)

                if not os.path.exists(tmp_file):
                    print(proj, bug_id, 'failed.', tmp_file, 'not found.')
                print(proj, bug_id, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                if result['input'].strip() == '':
                    print(proj, bug_id, 'failed. all empty.')

                result = json.load(open(tmp_file, 'r'))
                filename = f"{proj}_{bug_id}_{path}_{rem_loc}"
                codet5_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }

                self.command(['rm', '-rf', tmp_file])
                self.command(['rm', '-rf', tmp_dir])
                json.dump(codet5_input, open(output_file, 'w'), indent=2)
            self.command(['rm', '-rf', self.BENCH_DIR + f'tmp/defects4j/proj/'])

        else:
            raise "Bad benchmark specified"

    def create_output(self, input_file, output_file, tokenizer_dir, model_dir, model_name, max_new_tokens, num_output=10):
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, trust_remote_code=True)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_dir, trust_remote_code=True).to("cuda")
        codet5_output = json.load(open(input_file, 'r'))
        codet5_output['model'] = model_name
        start_time = time.time()
        for i, filename in enumerate(codet5_output['data']):
            text = codet5_output['data'][filename]['input']
            print(i + 1, 'generating', filename)

            try:
                inputs = tokenizer(text, return_tensors="pt").to("cuda")
                # https://huggingface.co/Salesforce/codet5p-220m/blob/main/tokenizer_config.json
                if inputs['input_ids'].size(1) >= 512:
                    print('input too long:', inputs['input_ids'].size(1), 'skip')
                    continue

                # https://github.com/salesforce/CodeT5/tree/main/CodeT5%2B
                if "codet5p" in model_dir:
                    inputs['decoder_input_ids'] = inputs['input_ids'].clone()
                generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=num_output, num_return_sequences=num_output, early_stopping=True)
                output = []
                for generated_id in generated_ids:
                    if "codet5p" in model_dir:
                        generated_tokens = tokenizer.decode(generated_id, skip_special_tokens=False)
                        start_idx = generated_tokens.find("<extra_id_0>")
                        end_idx = generated_tokens.find("<extra_id_1>")
                        generated_tokens = generated_tokens[start_idx:end_idx] if end_idx > 0 else generated_tokens[start_idx:]
                        generated_tokens = generated_tokens.replace("<extra_id_0>", "")
                        output.append(generated_tokens)
                    else:
                        output.append(tokenizer.decode(generated_id, skip_special_tokens=True))
            except Exception as e:
                print(f"Can't load the model, unexpected exception occured: {e}")
                output = []
            codet5_output['data'][filename]['output'] = output
            json.dump(codet5_output, open(output_file, 'w'), indent=2)
        total_time = int(time.time() - start_time)
        codet5_output['time'] = total_time
        json.dump(codet5_output, open(output_file, 'w'), indent=2)

    @staticmethod
    def output_to_patch(output, config):
        return output.strip()


StarCoderInputConfig = {
    "STARCODER_COMPLETE_CODEFORM_NOCOMMENT": {
        "model_id": "starcoderbase-1B/3B/7B",
        "input": "buggy function before",
        "patch": "code generated by the model, which will replace the entire buggy function. need extra analysis to figure out where to stop"
    },
    "STARCODER_COMPLETE_CODEFORM_COMMENTFORM_NOCOMMENT": {
        "model_id": "starcoderbase-1B/3B/7B",
        "input": "buggy function before",
        "patch": "the buggy function before the buggy lines, with buggy lines start with '// buggy line:'. remove all the other commonts and empty lines in the code"
    }
}

class StarCoder(Model):
    def __init__(self, JAVA_DIR, BENCH_DIR):
        super().__init__()
        self.JAVA_DIR = JAVA_DIR
        self.BENCH_DIR = BENCH_DIR

    def command(self, cmd):
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, err = process.communicate()
        if output != b'' or err != b'':
            print(output)
            print(err)
        return output, err

    def get_parsed_input(self, filename, start, end, config, tmp_file):
        os.chdir(self.JAVA_DIR)
        self.command([
            'java', '-cp', '.:target:lib/*', 'clm.starcoder.StarCoderInputParser',
            filename, start, end, config, tmp_file
        ])

    def get_input(self, config, output_file, bench_dir):
        if "humaneval" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'humaneval_loc.txt', 'r', 'utf-8')
            starcoder_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end

                tmp_file = self.BENCH_DIR + 'tmp/humaneval-java/proj/tmp_starcoder.json'
                self.get_parsed_input(
                    bench_dir + 'src/main/java/humaneval/buggy/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', tmp_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                starcoder_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(starcoder_input, open(output_file, 'w'), indent=2)

        elif "quixbugs" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'quixbugs_loc.txt', 'r', 'utf-8')
            starcoder_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end

                tmp_file = self.BENCH_DIR + 'tmp/quixbugs/proj/tmp_starcoder.json'
                self.get_parsed_input(
                    bench_dir + 'java_programs/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', tmp_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                starcoder_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(starcoder_input, open(output_file, 'w'), indent=2)

        elif "defects4j" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'defects4j_loc.txt', 'r', 'utf-8')
            starcoder_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                proj, bug_id, path, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end
                tmp_dir = self.BENCH_DIR + f'tmp/defects4j/proj/{proj}_{bug_id}/'
                tmp_file = f'{tmp_dir}/tmp_starcoder.json'

                subprocess.call(['defects4j', 'checkout', '-p', proj, '-v', bug_id + 'b', '-w', tmp_dir],
                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                self.get_parsed_input(tmp_dir + path, start, end, config, tmp_file)

                if not os.path.exists(tmp_file):
                    print(proj, bug_id, 'failed.', tmp_file, 'not found.')
                print(proj, bug_id, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                if result['input'].strip() == '':
                    print(proj, bug_id, 'failed. all empty.')

                result = json.load(open(tmp_file, 'r'))
                filename = f"{proj}_{bug_id}_{path}_{rem_loc}"
                starcoder_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }

                self.command(['rm', '-rf', tmp_file])
                self.command(['rm', '-rf', tmp_dir])
                json.dump(starcoder_input, open(output_file, 'w'), indent=2)
            self.command(['rm', '-rf', self.BENCH_DIR + f'tmp/defects4j/proj/'])

        else:
            raise "Bad benchmark specified"

    def create_output(self, input_file, output_file, tokenizer_dir, model_dir, model_name, max_new_tokens, num_output=10):
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, token=access_token)
        model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True, token=access_token).to("cuda")
        starcoder_output = json.load(open(input_file, 'r'))
        starcoder_output['model'] = model_name
        start_time = time.time()
        for i, filename in enumerate(starcoder_output['data']):
            text = starcoder_output['data'][filename]['input']
            print(i + 1, 'generating', filename)

            try:
                inputs = tokenizer(text, return_tensors="pt").to("cuda")
                # Original was 512
                if inputs['input_ids'].size(1) >= int(tokenizer.model_max_length):
                    print('input too long:', inputs['input_ids'].size(1), 'skip')
                    continue

                eos_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
                generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=num_output, num_return_sequences=num_output, early_stopping=True, eos_token_id=eos_id)
                output = []
                for generated_id in generated_ids:
                    generated_tokens = tokenizer.decode(generated_id, skip_special_tokens=False)
                    idx = generated_tokens.find("<fim_middle>")
                    generated_tokens = generated_tokens.replace("<|endoftext|>", "")
                    output.append(generated_tokens[idx + len("<fim_middle>"):])
            except Exception as e:
                print(f"Can't load the model, unexpected exception occured: {e}")
                output = []
            starcoder_output['data'][filename]['output'] = output
            json.dump(starcoder_output, open(output_file, 'w'), indent=2)
        total_time = int(time.time() - start_time)
        starcoder_output['time'] = total_time
        json.dump(starcoder_output, open(output_file, 'w'), indent=2)

    @staticmethod
    def output_to_patch(output, config):
        return output.strip()

    @staticmethod
    def prepare_input(fn_before, fn_bug, fn_fix, fn_after, eos_token):
        inputs = "<fim_prefix>" + fn_before
        if fn_bug is not None:
            # Replace all lines with buggy line format
            fn_bug = fn_bug.replace("\n", "\n// buggy line:")

            # Remove last buggy line (For empty line)
            if fn_bug.endswith('// buggy line:'):
                fn_bug = fn_bug[:-len(str('// buggy line:'))]

            # If buggy line is not already included
            if not fn_bug.startswith('\n// buggy line:'):
                inputs += '// buggy line: '

            # Add buggy part and special token
            inputs += fn_bug + "<fim_suffix>\n"
        inputs += fn_after + "<fim_middle>"
        outputs = fn_fix + eos_token
        return inputs, outputs


DeepSeekCoderInputConfig = {
    "DEEPSEEKCODER_COMPLETE_CODEFORM_NOCOMMENT": {
        "model_id": "deepseekcoder-base-1.3B/6.7B",
        "input": "buggy function before",
        "patch": "code generated by the model, which will replace the entire buggy function. need extra analysis to figure out where to stop"
    },
    "DEEPSEEKCODER_COMPLETE_CODEFORM_COMMENTFORM_NOCOMMENT": {
        "model_id": "deepseekcoder-base-1.3B/6.7B",
        "input": "buggy function before",
        "patch": "the buggy function before the buggy lines, with buggy lines start with '// buggy line:'. remove all the other commonts and empty lines in the code"
    }
}

class DeepSeekCoder(Model):
    def __init__(self, JAVA_DIR, BENCH_DIR):
        super().__init__()
        self.JAVA_DIR = JAVA_DIR
        self.BENCH_DIR = BENCH_DIR

    def command(self, cmd):
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, err = process.communicate()
        if output != b'' or err != b'':
            print(output)
            print(err)
        return output, err

    def get_parsed_input(self, filename, start, end, config, tmp_file):
        os.chdir(self.JAVA_DIR)
        self.command([
            'java', '-cp', '.:target:lib/*', 'clm.deepseekcoder.DeepSeekCoderInputParser',
            filename, start, end, config, tmp_file
        ])

    def get_input(self, config, output_file, bench_dir):
        if "humaneval" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'humaneval_loc.txt', 'r', 'utf-8')
            deepseekcoder_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end

                tmp_file = self.BENCH_DIR + 'tmp/humaneval-java/proj/tmp_deepseekcoder.json'
                self.get_parsed_input(
                    bench_dir + 'src/main/java/humaneval/buggy/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', tmp_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                deepseekcoder_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(deepseekcoder_input, open(output_file, 'w'), indent=2)

        elif "quixbugs" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'quixbugs_loc.txt', 'r', 'utf-8')
            deepseekcoder_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end

                tmp_file = self.BENCH_DIR + 'tmp/quixbugs/proj/tmp_deepseekcoder.json'
                self.get_parsed_input(
                    bench_dir + 'java_programs/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', tmp_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                deepseekcoder_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(deepseekcoder_input, open(output_file, 'w'), indent=2)

        elif "defects4j" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'defects4j_loc.txt', 'r', 'utf-8')
            deepseekcoder_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                proj, bug_id, path, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end
                tmp_dir = self.BENCH_DIR + f'tmp/defects4j/proj/{proj}_{bug_id}/'
                tmp_file = f'{tmp_dir}/tmp_deepseekcoder.json'

                subprocess.call(['defects4j', 'checkout', '-p', proj, '-v', bug_id + 'b', '-w', tmp_dir],
                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                self.get_parsed_input(tmp_dir + path, start, end, config, tmp_file)

                if not os.path.exists(tmp_file):
                    print(proj, bug_id, 'failed.', tmp_file, 'not found.')
                print(proj, bug_id, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                if result['input'].strip() == '':
                    print(proj, bug_id, 'failed. all empty.')

                result = json.load(open(tmp_file, 'r'))
                filename = f"{proj}_{bug_id}_{path}_{rem_loc}"
                deepseekcoder_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }

                self.command(['rm', '-rf', tmp_file])
                self.command(['rm', '-rf', tmp_dir])
                json.dump(deepseekcoder_input, open(output_file, 'w'), indent=2)
            self.command(['rm', '-rf', self.BENCH_DIR + f'tmp/defects4j/proj/'])

        else:
            raise "Bad benchmark specified"

    def create_output(self, input_file, output_file, tokenizer_dir, model_dir, model_name, max_new_tokens, num_output=10):
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
        model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True).to("cuda")
        deepseekcoder_output = json.load(open(input_file, 'r'))
        deepseekcoder_output['model'] = model_name
        start_time = time.time()
        for i, filename in enumerate(deepseekcoder_output['data']):
            text = deepseekcoder_output['data'][filename]['input']
            print(i + 1, 'generating', filename)

            try:
                inputs = tokenizer(text, return_tensors="pt").to("cuda")
                if inputs['input_ids'].size(1) >= int(tokenizer.model_max_length):
                    print('input too long:', inputs['input_ids'].size(1), 'skip')
                    continue

                eos_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
                generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=num_output, num_return_sequences=num_output,
                                            early_stopping=True, eos_token_id=eos_id)
                output = []
                for generated_id in generated_ids:
                    generated_tokens = tokenizer.decode(generated_id, skip_special_tokens=True)
                    output.append(generated_tokens[len(text):])
            except Exception as e:
                print(f"Can't load the model, unexpected exception occured: {e}")
                output = []
            deepseekcoder_output['data'][filename]['output'] = output
            json.dump(deepseekcoder_output, open(output_file, 'w'), indent=2)
        total_time = int(time.time() - start_time)
        deepseekcoder_output['time'] = total_time
        json.dump(deepseekcoder_output, open(output_file, 'w'), indent=2)

    @staticmethod
    def output_to_patch(output, config):
        return output.strip()

    @staticmethod
    def prepare_input(fn_before, fn_bug, fn_fix, fn_after, eos_token):
        inputs = fn_before + '<FILL_ME>' + fn_after
        outputs = fn_fix + tokenizer.eos_token


BloomInputConfig = {
    "BLOOM_COMPLETE_CODEFORM_NOCOMMENT": {
        "model_id": "bloom-base-560m/1.7B/7.1B",
        "input": "buggy function before",
        "patch": "code generated by the model, which will replace the entire buggy function. need extra analysis to figure out where to stop"
    },
    "BLOOM_COMPLETE_CODEFORM_COMMENTFORM_NOCOMMENT": {
        "model_id": "bloom-base-560m/1.7B/7.1B",
        "input": "buggy function before",
        "patch": "the buggy function before the buggy lines, with buggy lines start with '// buggy line:'. remove all the other commonts and empty lines in the code"
    }
}

class Bloom(Model):
    def __init__(self, JAVA_DIR, BENCH_DIR):
        super().__init__()
        self.JAVA_DIR = JAVA_DIR
        self.BENCH_DIR = BENCH_DIR

    def command(self, cmd):
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, err = process.communicate()
        if output != b'' or err != b'':
            print(output)
            print(err)
        return output, err

    def get_parsed_input(self, filename, start, end, config, tmp_file):
        os.chdir(self.JAVA_DIR)
        self.command([
            'java', '-cp', '.:target:lib/*', 'clm.bloom.BloomInputParser',
            filename, start, end, config, tmp_file
        ])

    def get_input(self, config, output_file, bench_dir):
        if "humaneval" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'humaneval_loc.txt', 'r', 'utf-8')
            bloom_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end

                tmp_file = self.BENCH_DIR + 'tmp/humaneval-java/proj/tmp_bloom.json'
                self.get_parsed_input(
                    bench_dir + 'src/main/java/humaneval/buggy/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', tmp_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                bloom_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(bloom_input, open(output_file, 'w'), indent=2)

        elif "quixbugs" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'quixbugs_loc.txt', 'r', 'utf-8')
            bloom_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end

                tmp_file = self.BENCH_DIR + 'tmp/quixbugs/proj/tmp_bloom.json'
                self.get_parsed_input(
                    bench_dir + 'java_programs/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', tmp_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                bloom_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(bloom_input, open(output_file, 'w'), indent=2)

        elif "defects4j" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'defects4j_loc.txt', 'r', 'utf-8')
            bloom_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                proj, bug_id, path, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end
                tmp_dir = self.BENCH_DIR + f'tmp/defects4j/proj/{proj}_{bug_id}/'
                tmp_file = f'{tmp_dir}/tmp_bloom.json'

                subprocess.call(['defects4j', 'checkout', '-p', proj, '-v', bug_id + 'b', '-w', tmp_dir],
                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                self.get_parsed_input(tmp_dir + path, start, end, config, tmp_file)

                if not os.path.exists(tmp_file):
                    print(proj, bug_id, 'failed.', tmp_file, 'not found.')
                print(proj, bug_id, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                if result['input'].strip() == '':
                    print(proj, bug_id, 'failed. all empty.')

                result = json.load(open(tmp_file, 'r'))
                filename = f"{proj}_{bug_id}_{path}_{rem_loc}"
                bloom_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }

                self.command(['rm', '-rf', tmp_file])
                self.command(['rm', '-rf', tmp_dir])
                json.dump(bloom_input, open(output_file, 'w'), indent=2)
            self.command(['rm', '-rf', self.BENCH_DIR + f'tmp/defects4j/proj/'])

        else:
            raise "Bad benchmark specified"

    def create_output(self, input_file, output_file, tokenizer_dir, model_dir, model_name, max_new_tokens, num_output=10):
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
        model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True).to("cuda")
        bloom_output = json.load(open(input_file, 'r'))
        bloom_output['model'] = model_name
        start_time = time.time()
        for i, filename in enumerate(bloom_output['data']):
            text = bloom_output['data'][filename]['input']
            print(i + 1, 'generating', filename)

            try:
                inputs = tokenizer(text, return_tensors="pt").to("cuda")
                # Original was 512
                if inputs['input_ids'].size(1) >= int(tokenizer.model_max_length):
                    print('input too long:', inputs['input_ids'].size(1), 'skip')
                    continue

                eos_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
                generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=num_output, num_return_sequences=num_output,
                                            early_stopping=True, eos_token_id=eos_id)
                output = []
                for generated_id in generated_ids:
                    output.append(tokenizer.decode(generated_id, truncate_before_pattern=[r"\n\n^#", "^'", "\n\n\n"], skip_special_tokens=True))
            except Exception as e:
                print(f"Can't load the model, unexpected exception occured: {e}")
                output = []
            bloom_output['data'][filename]['output'] = output
            json.dump(bloom_output, open(output_file, 'w'), indent=2)
        total_time = int(time.time() - start_time)
        bloom_output['time'] = total_time
        json.dump(bloom_output, open(output_file, 'w'), indent=2)

    @staticmethod
    def output_to_patch(output, config):
        """
        find the } that matches the first { in the output
        """
        output = output.strip().split('\n')
        no_comment_output = [line for line in output if not line.strip().startswith('//')]
        output = '\n'.join(no_comment_output)
        stack = ['{']
        try:
            start_index = output.index('{')
            patch = output[: start_index + 1]
            for c in output[start_index + 1:]:
                patch += c
                if c == '}':
                    top = stack.pop()
                    if top != '{':
                        return ''
                    if len(stack) == 0:
                        return patch.strip()
                elif c == '{':
                    stack.append(c)
            return ''
        except Exception as e:
            return ''

    @staticmethod
    def prepare_input(fn_before, fn_bug, fn_fix, fn_after, eos_token):
        inputs = fn_before + '<FILL_ME>' + fn_after
        outputs = fn_fix + tokenizer.eos_token


CodeLlamaInputConfig = {
    "CODELLAMA_COMPLETE_CODEFORM_NOCOMMENT": {
        "model_id": "CodeLlama-7b-hf",
        "input": "buggy function before",
        "patch": "code generated by the model, which will replace the entire buggy function. need extra analysis to figure out where to stop"
    },
    "CODELLAMA_COMPLETE_CODEFORM_COMMENTFORM_NOCOMMENT": {
        "model_id": "CodeLlama-7b-hf",
        "input": "buggy function before",
        "patch": "the buggy function before the buggy lines, with buggy lines start with '// buggy line:'. remove all the other commonts and empty lines in the code"
    }
}

class CodeLlama(Model):
    def __init__(self, JAVA_DIR, BENCH_DIR):
        super().__init__()
        self.JAVA_DIR = JAVA_DIR
        self.BENCH_DIR = BENCH_DIR

    def command(self, cmd):
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, err = process.communicate()
        if output != b'' or err != b'':
            print(output)
            print(err)
        return output, err

    def get_parsed_input(self, filename, start, end, config, tmp_file):
        os.chdir(self.JAVA_DIR)
        self.command([
            'java', '-cp', '.:target:lib/*', 'clm.codellama.CodeLlamaInputParser',
            filename, start, end, config, tmp_file
        ])

    def get_input(self, config, output_file, bench_dir):
        if "humaneval" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'humaneval_loc.txt', 'r', 'utf-8')
            codellama_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end

                tmp_file = self.BENCH_DIR + 'tmp/humaneval-java/proj/tmp_codellama.json'
                self.get_parsed_input(
                    bench_dir + 'src/main/java/humaneval/buggy/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', tmp_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                codellama_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(codellama_input, open(output_file, 'w'), indent=2)

        elif "quixbugs" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'quixbugs_loc.txt', 'r', 'utf-8')
            codellama_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                filename, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end

                tmp_file = self.BENCH_DIR + 'tmp/quixbugs/proj/tmp_codellama.json'
                self.get_parsed_input(
                    bench_dir + 'java_programs/' + filename + '.java',
                    start,
                    end,
                    config,
                    tmp_file
                )

                if not os.path.exists(tmp_file):
                    print(filename, 'failed.', tmp_file, 'not found.')
                print(filename, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                codellama_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }
                self.command(['rm', '-rf', tmp_file])
            json.dump(codellama_input, open(output_file, 'w'), indent=2)

        elif "defects4j" in bench_dir:
            loc_fp = codecs.open(bench_dir + 'defects4j_loc.txt', 'r', 'utf-8')
            codellama_input = {'config': config, 'data': {}}
            for line in loc_fp.readlines():
                proj, bug_id, path, rem_loc, add_loc = line.strip().split()
                start, end = rem_loc.split('-')
                end = str(int(end) - 1) if end != start else end
                tmp_dir = self.BENCH_DIR + f'tmp/defects4j/proj/{proj}_{bug_id}/'
                tmp_file = f'{tmp_dir}/tmp_codellama.json'

                subprocess.call(['defects4j', 'checkout', '-p', proj, '-v', bug_id + 'b', '-w', tmp_dir],
                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                self.get_parsed_input(tmp_dir + path, start, end, config, tmp_file)

                if not os.path.exists(tmp_file):
                    print(proj, bug_id, 'failed.', tmp_file, 'not found.')
                print(proj, bug_id, 'succeeded')

                result = json.load(open(tmp_file, 'r'))
                if result['input'].strip() == '':
                    print(proj, bug_id, 'failed. all empty.')

                result = json.load(open(tmp_file, 'r'))
                filename = f"{proj}_{bug_id}_{path}_{rem_loc}"
                codellama_input['data'][filename] = {
                    'loc': rem_loc,
                    'input': result['input'],
                    'function range': result['function range']
                }

                self.command(['rm', '-rf', tmp_file])
                self.command(['rm', '-rf', tmp_dir])
                json.dump(codellama_input, open(output_file, 'w'), indent=2)
            self.command(['rm', '-rf', self.BENCH_DIR + f'tmp/defects4j/proj/'])

        else:
            raise "Bad benchmark specified"

    def create_output(self, input_file, output_file, tokenizer_dir, model_dir, model_name, max_new_tokens, num_output=10):
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
        model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True).to("cuda")
        codellama_output = json.load(open(input_file, 'r'))
        codellama_output['model'] = model_name
        start_time = time.time()
        for i, filename in enumerate(codellama_output['data']):
            text = codellama_output['data'][filename]['input']
            print(i + 1, 'generating', filename)

            try:
                inputs = tokenizer(text, return_tensors="pt").to("cuda")
                if inputs['input_ids'].size(1) >= int(tokenizer.model_max_length):
                    print('input too long:', inputs['input_ids'].size(1), 'skip')
                    continue

                eos_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
                generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=num_output, num_return_sequences=num_output,
                                            early_stopping=True, pad_token_id=eos_id, eos_token_id=eos_id)
                output = []
                for generated_id in generated_ids:
                    o = tokenizer.decode(generated_id[inputs["input_ids"].size(1):], skip_special_tokens=True)
                    output.append(o)
            except Exception as e:
                print(f"Can't load the model, unexpected exception occured: {e}")
                output = []
            codellama_output['data'][filename]['output'] = output
            json.dump(codellama_output, open(output_file, 'w'), indent=2)
        total_time = int(time.time() - start_time)
        codellama_output['time'] = total_time
        json.dump(codellama_output, open(output_file, 'w'), indent=2)

    @staticmethod
    def output_to_patch(output, config):
        return output.strip()

    @staticmethod
    def prepare_input(fn_before, fn_bug, fn_fix, fn_after, eos_token):
        inputs = fn_before
        if fn_bug is not None:
            # Replace all lines with buggy line format
            fn_bug = fn_bug.replace("\n", "\n// buggy line:")

            # Remove last buggy line (For empty line)
            if fn_bug.endswith('// buggy line:'):
                fn_bug = fn_bug[:-len(str('// buggy line:'))]

            # If buggy line is not already included
            if not fn_bug.startswith('\n// buggy line:'):
                inputs += '// buggy line: '

            # Add buggy part and special token
            inputs += fn_bug + "<FILL_ME>\n"
        inputs += fn_after
        outputs = fn_fix + "<EOT>"
        return inputs, outputs


# Models dictionary
models_classes = {}
models_classes['codet5p'] = {'model': CodeT5}
models_classes['codegen'] = {'model': CodeGen}
models_classes['starcoder'] = {'model': StarCoder}
models_classes['deepseekcoder'] = {'model': DeepSeekCoder}
models_classes['bloom'] = {'model': Bloom}
models_classes['codellama'] = {'model': CodeLlama}

# Training dictionary
training_classes = {}
training_classes['codet5p'] = {'tokenizer': AutoTokenizer, 'model': AutoModelForSeq2SeqLM}
training_classes['codegen'] = {'tokenizer': AutoTokenizer, 'model': AutoModelForCausalLM}
training_classes['starcoder'] = {'tokenizer': AutoTokenizer, 'model': AutoModelForCausalLM}
training_classes['deepseekcoder'] = {'tokenizer': AutoTokenizer, 'model': AutoModelForCausalLM}
training_classes['bloom'] = {'tokenizer': AutoTokenizer, 'model': AutoModelForCausalLM}
training_classes['codellama'] = {'tokenizer': AutoTokenizer, 'model': AutoModelForCausalLM}
